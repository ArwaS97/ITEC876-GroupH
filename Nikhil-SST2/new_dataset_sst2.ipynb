{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Predicting Movie Reviews with BERT on TF Hub.ipynb","provenance":[{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1573602835281}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"j0a4mTk9o1Qg","colab_type":"code","colab":{}},"source":["# Copyright 2019 Google Inc.\n","\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"source":["#Predicting Movie Review Sentiment with BERT on TF Hub"]},{"cell_type":"markdown","metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"source":["If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n","\n","Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n","\n","Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"]},{"cell_type":"code","metadata":{"id":"hsZvic2YxnTz","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cp5wfXDx5SPH","colab_type":"text"},"source":["In addition to the standard libraries we imported above, we'll need to install BERT's python package."]},{"cell_type":"code","metadata":{"id":"jviywGyWyKsA","colab_type":"code","outputId":"abec6edf-9402-468e-e0dd-67795e03a600","colab":{"base_uri":"https://localhost:8080/","height":123},"executionInfo":{"status":"ok","timestamp":1573606676083,"user_tz":-660,"elapsed":8087,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}}},"source":["!pip install bert-tensorflow"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |████▉                           | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.1MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hhbGEfwgdEtw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"5158fb08-5578-4f56-9d04-a40f33de9288","executionInfo":{"status":"ok","timestamp":1573606676084,"user_tz":-660,"elapsed":8048,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}}},"source":["import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":29,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"source":["Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n","\n","Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"cell_type":"code","metadata":{"id":"US_EAnICvP7f","colab_type":"code","outputId":"e9abf5bc-eb0b-47a8-bb44-e93187c61d5f","cellView":"form","executionInfo":{"status":"ok","timestamp":1573606710450,"user_tz":-660,"elapsed":42361,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Set the output directory for saving model file\n","# Optionally, set a GCP bucket location\n","\n","OUTPUT_DIR = 'OUTPUT_DIR_NAME'#@param {type:\"string\"}\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = False #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = True #@param {type:\"boolean\"}\n","BUCKET = 'sst2-ads-project' #@param {type:\"string\"}\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["***** Model output directory: gs://sst2-ads-project/OUTPUT_DIR_NAME *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"source":["#Data"]},{"cell_type":"markdown","metadata":{"id":"MC_w8SRqN0fr","colab_type":"text"},"source":["First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."]},{"cell_type":"code","metadata":{"id":"fom_ff20gyy6","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","import os\n","import re\n","\n","# Load all files from a directory in a DataFrame.\n","def load_directory_data(directory):\n","  data = {}\n","  data[\"sentence\"] = []\n","  data[\"sentiment\"] = []\n","  for file_path in os.listdir(directory):\n","    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n","      data[\"sentence\"].append(f.read())\n","      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n","  return pd.DataFrame.from_dict(data)\n","\n","# Merge positive and negative examples, add a polarity column and shuffle.\n","def load_dataset(directory):\n","  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n","  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n","  pos_df[\"polarity\"] = 1\n","  neg_df[\"polarity\"] = 0\n","  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","  dataset = tf.keras.utils.get_file(\n","      fname=\"New Text Document.txt\", \n","      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n","      extract=True)\n","  \n","  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                       \"aclImdb\", \"train\"))\n","  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                      \"aclImdb\", \"test\"))\n","  \n","  return train_df, test_df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2abfwdn-g135","colab_type":"code","outputId":"dcf62d9e-9b44-4719-f65b-ee0337caac0a","executionInfo":{"status":"ok","timestamp":1573606756590,"user_tz":-660,"elapsed":88405,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["train, test = download_and_load_datasets()"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","84131840/84125825 [==============================] - 8s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XA8WHJgzhIZf","colab_type":"text"},"source":["To keep training fast, we'll take a sample of 5000 train and test examples, respectively."]},{"cell_type":"code","metadata":{"id":"lw_F488eixTV","colab_type":"code","colab":{}},"source":["train = train.sample(5000)\n","test = test.sample(5000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"prRQM8pDi8xI","colab_type":"code","outputId":"cdf091cf-c065-4452-ac32-fb8c2875e401","executionInfo":{"status":"ok","timestamp":1573606756594,"user_tz":-660,"elapsed":88356,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["train.columns"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['sentence', 'sentiment', 'polarity'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"sfRnHSz3iSXz","colab_type":"text"},"source":["For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","colab":{}},"source":["DATA_COLUMN = 'sentence'\n","LABEL_COLUMN = 'polarity'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"source":["#Data Preprocessing\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. True, False"]},{"cell_type":"code","metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"source":["Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n","\n","\n","1. Lowercase our text (if we're using a BERT lowercase model)\n","2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","4. Map our words to indexes using a vocab file that BERT provides\n","5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qMWiDtpyQSoU","colab_type":"text"},"source":["To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"]},{"cell_type":"code","metadata":{"id":"IhJSe0QHNG7U","colab_type":"code","outputId":"7f844d35-197f-470f-8e89-3f9b3c92825c","executionInfo":{"status":"ok","timestamp":1573606770799,"user_tz":-660,"elapsed":102521,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# This is a path to an uncased (all lowercase) version of BERT\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","def create_tokenizer_from_hub_module():\n","  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","  with tf.Graph().as_default():\n","    bert_module = hub.Module(BERT_MODEL_HUB)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","      \n","  return bert.tokenization.FullTokenizer(\n","      vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","tokenizer = create_tokenizer_from_hub_module()"],"execution_count":37,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"z4oFkhpZBDKm","colab_type":"text"},"source":["Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"]},{"cell_type":"code","metadata":{"id":"dsBo6RCtQmwx","colab_type":"code","outputId":"c21d5668-4e02-4a70-ddd5-bdeb58e41380","executionInfo":{"status":"ok","timestamp":1573606770801,"user_tz":-660,"elapsed":102502,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"cell_type":"code","metadata":{"id":"LL5W8gEGRTAf","colab_type":"code","outputId":"e1e6b2cb-9372-4bbc-bd58-c927dcda35da","executionInfo":{"status":"ok","timestamp":1573606809567,"user_tz":-660,"elapsed":141253,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# We'll set sequences to be at most 128 tokens long.\n","MAX_SEQ_LENGTH = 128\n","# Convert our train and test features to InputFeatures that BERT understands.\n","train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] many people have commented that this movie was nowhere near as good as the first . well , maybe it isn ' t - to you . however , how does your child react to it ? well , mine loved it more than the first . < br / > < br / > disney movies of the past can sometimes be a little harsh for little kids . ( for example - bam ##bi ' s mother getting shot . ) this movie was really great for my sensitive little girl who likes humor and happy endings . < br / > < br / > if you want to be s ##nob ##by about what should be disney ' s standards based on [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] many people have commented that this movie was nowhere near as good as the first . well , maybe it isn ' t - to you . however , how does your child react to it ? well , mine loved it more than the first . < br / > < br / > disney movies of the past can sometimes be a little harsh for little kids . ( for example - bam ##bi ' s mother getting shot . ) this movie was really great for my sensitive little girl who likes humor and happy endings . < br / > < br / > if you want to be s ##nob ##by about what should be disney ' s standards based on [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2116 2111 2031 7034 2008 2023 3185 2001 7880 2379 2004 2204 2004 1996 2034 1012 2092 1010 2672 2009 3475 1005 1056 1011 2000 2017 1012 2174 1010 2129 2515 2115 2775 10509 2000 2009 1029 2092 1010 3067 3866 2009 2062 2084 1996 2034 1012 1026 7987 1013 1028 1026 7987 1013 1028 6373 5691 1997 1996 2627 2064 2823 2022 1037 2210 8401 2005 2210 4268 1012 1006 2005 2742 1011 25307 5638 1005 1055 2388 2893 2915 1012 1007 2023 3185 2001 2428 2307 2005 2026 7591 2210 2611 2040 7777 8562 1998 3407 21306 1012 1026 7987 1013 1028 1026 7987 1013 1028 2065 2017 2215 2000 2022 1055 25083 3762 2055 2054 2323 2022 6373 1005 1055 4781 2241 2006 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2116 2111 2031 7034 2008 2023 3185 2001 7880 2379 2004 2204 2004 1996 2034 1012 2092 1010 2672 2009 3475 1005 1056 1011 2000 2017 1012 2174 1010 2129 2515 2115 2775 10509 2000 2009 1029 2092 1010 3067 3866 2009 2062 2084 1996 2034 1012 1026 7987 1013 1028 1026 7987 1013 1028 6373 5691 1997 1996 2627 2064 2823 2022 1037 2210 8401 2005 2210 4268 1012 1006 2005 2742 1011 25307 5638 1005 1055 2388 2893 2915 1012 1007 2023 3185 2001 2428 2307 2005 2026 7591 2210 2611 2040 7777 8562 1998 3407 21306 1012 1026 7987 1013 1028 1026 7987 1013 1028 2065 2017 2215 2000 2022 1055 25083 3762 2055 2054 2323 2022 6373 1005 1055 4781 2241 2006 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i caught this at a screening at the sundance film festival and was in awe over the absolute power this film has . it is an examination of the psychological effects on our brave soldiers who join the military with hopes that they will protect and serve our country with honor as well as be taken care of by our government for it . the film details the psychological changes that takes place in boot camp as the soldiers are turned into \" killers for their country \" and put into the war and the after effects once they return home . it also portrays the effect that killing has on the human psyche . it pays homage to the soldiers and never ever critic ##izes [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i caught this at a screening at the sundance film festival and was in awe over the absolute power this film has . it is an examination of the psychological effects on our brave soldiers who join the military with hopes that they will protect and serve our country with honor as well as be taken care of by our government for it . the film details the psychological changes that takes place in boot camp as the soldiers are turned into \" killers for their country \" and put into the war and the after effects once they return home . it also portrays the effect that killing has on the human psyche . it pays homage to the soldiers and never ever critic ##izes [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 3236 2023 2012 1037 11326 2012 1996 20140 2143 2782 1998 2001 1999 15180 2058 1996 7619 2373 2023 2143 2038 1012 2009 2003 2019 7749 1997 1996 8317 3896 2006 2256 9191 3548 2040 3693 1996 2510 2007 8069 2008 2027 2097 4047 1998 3710 2256 2406 2007 3932 2004 2092 2004 2022 2579 2729 1997 2011 2256 2231 2005 2009 1012 1996 2143 4751 1996 8317 3431 2008 3138 2173 1999 9573 3409 2004 1996 3548 2024 2357 2046 1000 15978 2005 2037 2406 1000 1998 2404 2046 1996 2162 1998 1996 2044 3896 2320 2027 2709 2188 1012 2009 2036 17509 1996 3466 2008 4288 2038 2006 1996 2529 25774 1012 2009 12778 14822 2000 1996 3548 1998 2196 2412 6232 10057 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 3236 2023 2012 1037 11326 2012 1996 20140 2143 2782 1998 2001 1999 15180 2058 1996 7619 2373 2023 2143 2038 1012 2009 2003 2019 7749 1997 1996 8317 3896 2006 2256 9191 3548 2040 3693 1996 2510 2007 8069 2008 2027 2097 4047 1998 3710 2256 2406 2007 3932 2004 2092 2004 2022 2579 2729 1997 2011 2256 2231 2005 2009 1012 1996 2143 4751 1996 8317 3431 2008 3138 2173 1999 9573 3409 2004 1996 3548 2024 2357 2046 1000 15978 2005 2037 2406 1000 1998 2404 2046 1996 2162 1998 1996 2044 3896 2320 2027 2709 2188 1012 2009 2036 17509 1996 3466 2008 4288 2038 2006 1996 2529 25774 1012 2009 12778 14822 2000 1996 3548 1998 2196 2412 6232 10057 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] fi ##do is a story about more well manner ##ed zombies who have been trained to serve the human race . all falls apart though , when young timmy ' s zombie fi ##do eats the family neighbor . from then on , disaster ( well , maybe not disaster , but to some extent , chaos ) occurs . most of the people treat their zombies with fairness , and one such character sleeps with his zombie ( very funny part of the movie , if not also very disturbing too ) . and we find the loving fi ##do whatever he may do . this is a very funny and unique film , especially for the zombie genre . it is also probably one [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] fi ##do is a story about more well manner ##ed zombies who have been trained to serve the human race . all falls apart though , when young timmy ' s zombie fi ##do eats the family neighbor . from then on , disaster ( well , maybe not disaster , but to some extent , chaos ) occurs . most of the people treat their zombies with fairness , and one such character sleeps with his zombie ( very funny part of the movie , if not also very disturbing too ) . and we find the loving fi ##do whatever he may do . this is a very funny and unique film , especially for the zombie genre . it is also probably one [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 10882 3527 2003 1037 2466 2055 2062 2092 5450 2098 14106 2040 2031 2042 4738 2000 3710 1996 2529 2679 1012 2035 4212 4237 2295 1010 2043 2402 27217 1005 1055 11798 10882 3527 20323 1996 2155 11429 1012 2013 2059 2006 1010 7071 1006 2092 1010 2672 2025 7071 1010 2021 2000 2070 6698 1010 8488 1007 5158 1012 2087 1997 1996 2111 7438 2037 14106 2007 26935 1010 1998 2028 2107 2839 25126 2007 2010 11798 1006 2200 6057 2112 1997 1996 3185 1010 2065 2025 2036 2200 14888 2205 1007 1012 1998 2057 2424 1996 8295 10882 3527 3649 2002 2089 2079 1012 2023 2003 1037 2200 6057 1998 4310 2143 1010 2926 2005 1996 11798 6907 1012 2009 2003 2036 2763 2028 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 10882 3527 2003 1037 2466 2055 2062 2092 5450 2098 14106 2040 2031 2042 4738 2000 3710 1996 2529 2679 1012 2035 4212 4237 2295 1010 2043 2402 27217 1005 1055 11798 10882 3527 20323 1996 2155 11429 1012 2013 2059 2006 1010 7071 1006 2092 1010 2672 2025 7071 1010 2021 2000 2070 6698 1010 8488 1007 5158 1012 2087 1997 1996 2111 7438 2037 14106 2007 26935 1010 1998 2028 2107 2839 25126 2007 2010 11798 1006 2200 6057 2112 1997 1996 3185 1010 2065 2025 2036 2200 14888 2205 1007 1012 1998 2057 2424 1996 8295 10882 3527 3649 2002 2089 2079 1012 2023 2003 1037 2200 6057 1998 4310 2143 1010 2926 2005 1996 11798 6907 1012 2009 2003 2036 2763 2028 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] love the characters and the story line . very funny with plenty of action . thomas ian griffith and tia carr ##er give great performances . i enjoyed the dynamic and comical interaction of griffith and career . donald south ##erland plays a very li ##ka ##ble , and surprisingly sympathetic , burnt out hit man . all three actors are among my favorites and having them in this movie made for a special treat . a nice addition to my extensive dvd collection . i highly recommend this movie . if you like . . . mr . and mrs . smith , the replacement killers , l . a . confidential , the long kiss goodnight , the abyss and , the whole nine [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] love the characters and the story line . very funny with plenty of action . thomas ian griffith and tia carr ##er give great performances . i enjoyed the dynamic and comical interaction of griffith and career . donald south ##erland plays a very li ##ka ##ble , and surprisingly sympathetic , burnt out hit man . all three actors are among my favorites and having them in this movie made for a special treat . a nice addition to my extensive dvd collection . i highly recommend this movie . if you like . . . mr . and mrs . smith , the replacement killers , l . a . confidential , the long kiss goodnight , the abyss and , the whole nine [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2293 1996 3494 1998 1996 2466 2240 1012 2200 6057 2007 7564 1997 2895 1012 2726 4775 14135 1998 27339 12385 2121 2507 2307 4616 1012 1045 5632 1996 8790 1998 29257 8290 1997 14135 1998 2476 1012 6221 2148 22492 3248 1037 2200 5622 2912 3468 1010 1998 10889 13026 1010 11060 2041 2718 2158 1012 2035 2093 5889 2024 2426 2026 20672 1998 2383 2068 1999 2023 3185 2081 2005 1037 2569 7438 1012 1037 3835 2804 2000 2026 4866 4966 3074 1012 1045 3811 16755 2023 3185 1012 2065 2017 2066 1012 1012 1012 2720 1012 1998 3680 1012 3044 1010 1996 6110 15978 1010 1048 1012 1037 1012 18777 1010 1996 2146 3610 22708 1010 1996 22159 1998 1010 1996 2878 3157 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2293 1996 3494 1998 1996 2466 2240 1012 2200 6057 2007 7564 1997 2895 1012 2726 4775 14135 1998 27339 12385 2121 2507 2307 4616 1012 1045 5632 1996 8790 1998 29257 8290 1997 14135 1998 2476 1012 6221 2148 22492 3248 1037 2200 5622 2912 3468 1010 1998 10889 13026 1010 11060 2041 2718 2158 1012 2035 2093 5889 2024 2426 2026 20672 1998 2383 2068 1999 2023 3185 2081 2005 1037 2569 7438 1012 1037 3835 2804 2000 2026 4866 4966 3074 1012 1045 3811 16755 2023 3185 1012 2065 2017 2066 1012 1012 1012 2720 1012 1998 3680 1012 3044 1010 1996 6110 15978 1010 1048 1012 1037 1012 18777 1010 1996 2146 3610 22708 1010 1996 22159 1998 1010 1996 2878 3157 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] yes , the plot is hardly plausible and very thin . yes , the acting does range from average to laugh ##able . yes , it has been done so many times before . however what we are dealing with is a film that does not shy away from these facts and pretend ##s to be nothing more than it is . there are indeed some original death scenes and the tension does increase throughout the movie . in addition you are never more than a few minutes away from a go ##ry killing . i urge everyone to watch this film with an un ##pre ##ju ##dice ##d eye and see it for what it set out to be ; a scary , funny slash [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] yes , the plot is hardly plausible and very thin . yes , the acting does range from average to laugh ##able . yes , it has been done so many times before . however what we are dealing with is a film that does not shy away from these facts and pretend ##s to be nothing more than it is . there are indeed some original death scenes and the tension does increase throughout the movie . in addition you are never more than a few minutes away from a go ##ry killing . i urge everyone to watch this film with an un ##pre ##ju ##dice ##d eye and see it for what it set out to be ; a scary , funny slash [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2748 1010 1996 5436 2003 6684 24286 1998 2200 4857 1012 2748 1010 1996 3772 2515 2846 2013 2779 2000 4756 3085 1012 2748 1010 2009 2038 2042 2589 2061 2116 2335 2077 1012 2174 2054 2057 2024 7149 2007 2003 1037 2143 2008 2515 2025 11004 2185 2013 2122 8866 1998 9811 2015 2000 2022 2498 2062 2084 2009 2003 1012 2045 2024 5262 2070 2434 2331 5019 1998 1996 6980 2515 3623 2802 1996 3185 1012 1999 2804 2017 2024 2196 2062 2084 1037 2261 2781 2185 2013 1037 2175 2854 4288 1012 1045 9075 3071 2000 3422 2023 2143 2007 2019 4895 28139 9103 24598 2094 3239 1998 2156 2009 2005 2054 2009 2275 2041 2000 2022 1025 1037 12459 1010 6057 18296 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2748 1010 1996 5436 2003 6684 24286 1998 2200 4857 1012 2748 1010 1996 3772 2515 2846 2013 2779 2000 4756 3085 1012 2748 1010 2009 2038 2042 2589 2061 2116 2335 2077 1012 2174 2054 2057 2024 7149 2007 2003 1037 2143 2008 2515 2025 11004 2185 2013 2122 8866 1998 9811 2015 2000 2022 2498 2062 2084 2009 2003 1012 2045 2024 5262 2070 2434 2331 5019 1998 1996 6980 2515 3623 2802 1996 3185 1012 1999 2804 2017 2024 2196 2062 2084 1037 2261 2781 2185 2013 1037 2175 2854 4288 1012 1045 9075 3071 2000 3422 2023 2143 2007 2019 4895 28139 9103 24598 2094 3239 1998 2156 2009 2005 2054 2009 2275 2041 2000 2022 1025 1037 12459 1010 6057 18296 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] see dick work . < br / > < br / > see jane work . < br / > < br / > dick and jane are married . < br / > < br / > they are successful . < br / > < br / > they have a son . < br / > < br / > they have a nice house . < br / > < br / > they have a latino housekeeper . < br / > < br / > the housekeeper teaches spanish to the son . < br / > < br / > the son speaks spanish . < br / > < br / > ha - ha . < br [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] see dick work . < br / > < br / > see jane work . < br / > < br / > dick and jane are married . < br / > < br / > they are successful . < br / > < br / > they have a son . < br / > < br / > they have a nice house . < br / > < br / > they have a latino housekeeper . < br / > < br / > the housekeeper teaches spanish to the son . < br / > < br / > the son speaks spanish . < br / > < br / > ha - ha . < br [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2156 5980 2147 1012 1026 7987 1013 1028 1026 7987 1013 1028 2156 4869 2147 1012 1026 7987 1013 1028 1026 7987 1013 1028 5980 1998 4869 2024 2496 1012 1026 7987 1013 1028 1026 7987 1013 1028 2027 2024 3144 1012 1026 7987 1013 1028 1026 7987 1013 1028 2027 2031 1037 2365 1012 1026 7987 1013 1028 1026 7987 1013 1028 2027 2031 1037 3835 2160 1012 1026 7987 1013 1028 1026 7987 1013 1028 2027 2031 1037 7402 22583 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 22583 12011 3009 2000 1996 2365 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2365 8847 3009 1012 1026 7987 1013 1028 1026 7987 1013 1028 5292 1011 5292 1012 1026 7987 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2156 5980 2147 1012 1026 7987 1013 1028 1026 7987 1013 1028 2156 4869 2147 1012 1026 7987 1013 1028 1026 7987 1013 1028 5980 1998 4869 2024 2496 1012 1026 7987 1013 1028 1026 7987 1013 1028 2027 2024 3144 1012 1026 7987 1013 1028 1026 7987 1013 1028 2027 2031 1037 2365 1012 1026 7987 1013 1028 1026 7987 1013 1028 2027 2031 1037 3835 2160 1012 1026 7987 1013 1028 1026 7987 1013 1028 2027 2031 1037 7402 22583 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 22583 12011 3009 2000 1996 2365 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2365 8847 3009 1012 1026 7987 1013 1028 1026 7987 1013 1028 5292 1011 5292 1012 1026 7987 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i gave this movie a rating of 1 because it is by far , the worst movie i ' ve ever seen in my life . this movie was made in 2003 and i ' ve seen movies made in the 60 ' s with better special effects . i wish i could go into detail , but words can ' t describe how crap ##py this movie was . i could have done better with a home video camera and $ 20 ! i pray that chuck norris never makes a movie again . now if you think i ' m downing this movie because it has a christian theme , you ' re wrong . i like the fact that il ' chuck decided [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i gave this movie a rating of 1 because it is by far , the worst movie i ' ve ever seen in my life . this movie was made in 2003 and i ' ve seen movies made in the 60 ' s with better special effects . i wish i could go into detail , but words can ' t describe how crap ##py this movie was . i could have done better with a home video camera and $ 20 ! i pray that chuck norris never makes a movie again . now if you think i ' m downing this movie because it has a christian theme , you ' re wrong . i like the fact that il ' chuck decided [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2435 2023 3185 1037 5790 1997 1015 2138 2009 2003 2011 2521 1010 1996 5409 3185 1045 1005 2310 2412 2464 1999 2026 2166 1012 2023 3185 2001 2081 1999 2494 1998 1045 1005 2310 2464 5691 2081 1999 1996 3438 1005 1055 2007 2488 2569 3896 1012 1045 4299 1045 2071 2175 2046 6987 1010 2021 2616 2064 1005 1056 6235 2129 10231 7685 2023 3185 2001 1012 1045 2071 2031 2589 2488 2007 1037 2188 2678 4950 1998 1002 2322 999 1045 11839 2008 8057 15466 2196 3084 1037 3185 2153 1012 2085 2065 2017 2228 1045 1005 1049 22501 2023 3185 2138 2009 2038 1037 3017 4323 1010 2017 1005 2128 3308 1012 1045 2066 1996 2755 2008 6335 1005 8057 2787 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2435 2023 3185 1037 5790 1997 1015 2138 2009 2003 2011 2521 1010 1996 5409 3185 1045 1005 2310 2412 2464 1999 2026 2166 1012 2023 3185 2001 2081 1999 2494 1998 1045 1005 2310 2464 5691 2081 1999 1996 3438 1005 1055 2007 2488 2569 3896 1012 1045 4299 1045 2071 2175 2046 6987 1010 2021 2616 2064 1005 1056 6235 2129 10231 7685 2023 3185 2001 1012 1045 2071 2031 2589 2488 2007 1037 2188 2678 4950 1998 1002 2322 999 1045 11839 2008 8057 15466 2196 3084 1037 3185 2153 1012 2085 2065 2017 2228 1045 1005 1049 22501 2023 3185 2138 2009 2038 1037 3017 4323 1010 2017 1005 2128 3308 1012 1045 2066 1996 2755 2008 6335 1005 8057 2787 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this was a great anime . true the animation is old but its still worth watching and has a better plot than ninja scroll , the problem that it was kinda long . < br / > < br / > japanese movie star hi ##roy ##uki san ##ada who played u ##ji ##o from last samurai played the main character ji ##ro and it was directed by ri ##nta ##ro who did galaxy express 999 and metropolis . < br / > < br / > the anime has some good animation for an old anime , interesting characters like the main villain ten ##kai and and ##o sho ##uz ##an and of course lets not forget the beautiful musical scores in the film . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this was a great anime . true the animation is old but its still worth watching and has a better plot than ninja scroll , the problem that it was kinda long . < br / > < br / > japanese movie star hi ##roy ##uki san ##ada who played u ##ji ##o from last samurai played the main character ji ##ro and it was directed by ri ##nta ##ro who did galaxy express 999 and metropolis . < br / > < br / > the anime has some good animation for an old anime , interesting characters like the main villain ten ##kai and and ##o sho ##uz ##an and of course lets not forget the beautiful musical scores in the film . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 2001 1037 2307 8750 1012 2995 1996 7284 2003 2214 2021 2049 2145 4276 3666 1998 2038 1037 2488 5436 2084 14104 17186 1010 1996 3291 2008 2009 2001 17704 2146 1012 1026 7987 1013 1028 1026 7987 1013 1028 2887 3185 2732 7632 13238 14228 2624 8447 2040 2209 1057 4478 2080 2013 2197 16352 2209 1996 2364 2839 10147 3217 1998 2009 2001 2856 2011 15544 12380 3217 2040 2106 9088 4671 25897 1998 18236 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 8750 2038 2070 2204 7284 2005 2019 2214 8750 1010 5875 3494 2066 1996 2364 12700 2702 11151 1998 1998 2080 26822 17040 2319 1998 1997 2607 11082 2025 5293 1996 3376 3315 7644 1999 1996 2143 1012 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 2001 1037 2307 8750 1012 2995 1996 7284 2003 2214 2021 2049 2145 4276 3666 1998 2038 1037 2488 5436 2084 14104 17186 1010 1996 3291 2008 2009 2001 17704 2146 1012 1026 7987 1013 1028 1026 7987 1013 1028 2887 3185 2732 7632 13238 14228 2624 8447 2040 2209 1057 4478 2080 2013 2197 16352 2209 1996 2364 2839 10147 3217 1998 2009 2001 2856 2011 15544 12380 3217 2040 2106 9088 4671 25897 1998 18236 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 8750 2038 2070 2204 7284 2005 2019 2214 8750 1010 5875 3494 2066 1996 2364 12700 2702 11151 1998 1998 2080 26822 17040 2319 1998 1997 2607 11082 2025 5293 1996 3376 3315 7644 1999 1996 2143 1012 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film quickly gets to a major chase scene with ever increasing destruction . the first really bad thing is the guy hi ##jack ##ing steven sea ##gal would have been beaten to pulp by sea ##gal ' s driving , but that probably would have ended the whole premise for the movie . < br / > < br / > it seems like they decided to make all kinds of changes in the movie plot , so just plan to enjoy the action , and do not expect a coherent plot . turn any sense of logic you may have , it will reduce your chance of getting a headache . < br / > < br / > i does give me some [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film quickly gets to a major chase scene with ever increasing destruction . the first really bad thing is the guy hi ##jack ##ing steven sea ##gal would have been beaten to pulp by sea ##gal ' s driving , but that probably would have ended the whole premise for the movie . < br / > < br / > it seems like they decided to make all kinds of changes in the movie plot , so just plan to enjoy the action , and do not expect a coherent plot . turn any sense of logic you may have , it will reduce your chance of getting a headache . < br / > < br / > i does give me some [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2855 4152 2000 1037 2350 5252 3496 2007 2412 4852 6215 1012 1996 2034 2428 2919 2518 2003 1996 3124 7632 17364 2075 7112 2712 9692 2052 2031 2042 7854 2000 16016 2011 2712 9692 1005 1055 4439 1010 2021 2008 2763 2052 2031 3092 1996 2878 18458 2005 1996 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 3849 2066 2027 2787 2000 2191 2035 7957 1997 3431 1999 1996 3185 5436 1010 2061 2074 2933 2000 5959 1996 2895 1010 1998 2079 2025 5987 1037 18920 5436 1012 2735 2151 3168 1997 7961 2017 2089 2031 1010 2009 2097 5547 2115 3382 1997 2893 1037 14978 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2515 2507 2033 2070 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2855 4152 2000 1037 2350 5252 3496 2007 2412 4852 6215 1012 1996 2034 2428 2919 2518 2003 1996 3124 7632 17364 2075 7112 2712 9692 2052 2031 2042 7854 2000 16016 2011 2712 9692 1005 1055 4439 1010 2021 2008 2763 2052 2031 3092 1996 2878 18458 2005 1996 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 3849 2066 2027 2787 2000 2191 2035 7957 1997 3431 1999 1996 3185 5436 1010 2061 2074 2933 2000 5959 1996 2895 1010 1998 2079 2025 5987 1037 18920 5436 1012 2735 2151 3168 1997 7961 2017 2089 2031 1010 2009 2097 5547 2115 3382 1997 2893 1037 14978 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2515 2507 2033 2070 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] my architect is a great film about nathaniel kahn ' s search for himself via the legacy of his famous architect father , louis kahn , dead since 1974 . the film builds slowly , but perfectly , and what starts out as a seemingly lost forty ##some ##thing ' s identity crisis un ##fold ##s into a beautiful tale with much deeper meaning with regard to the importance of love , loss , family and perhaps more importantly , our life ' s work . < br / > < br / > i had never heard of louis kahn prior to this film , although i was vaguely familiar with some of his work . through the words ( both good and bad ) [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] my architect is a great film about nathaniel kahn ' s search for himself via the legacy of his famous architect father , louis kahn , dead since 1974 . the film builds slowly , but perfectly , and what starts out as a seemingly lost forty ##some ##thing ' s identity crisis un ##fold ##s into a beautiful tale with much deeper meaning with regard to the importance of love , loss , family and perhaps more importantly , our life ' s work . < br / > < br / > i had never heard of louis kahn prior to this film , although i was vaguely familiar with some of his work . through the words ( both good and bad ) [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2026 4944 2003 1037 2307 2143 2055 13961 19361 1005 1055 3945 2005 2370 3081 1996 8027 1997 2010 3297 4944 2269 1010 3434 19361 1010 2757 2144 3326 1012 1996 2143 16473 3254 1010 2021 6669 1010 1998 2054 4627 2041 2004 1037 9428 2439 5659 14045 20744 1005 1055 4767 5325 4895 10371 2015 2046 1037 3376 6925 2007 2172 6748 3574 2007 7634 2000 1996 5197 1997 2293 1010 3279 1010 2155 1998 3383 2062 14780 1010 2256 2166 1005 1055 2147 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2018 2196 2657 1997 3434 19361 3188 2000 2023 2143 1010 2348 1045 2001 15221 5220 2007 2070 1997 2010 2147 1012 2083 1996 2616 1006 2119 2204 1998 2919 1007 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2026 4944 2003 1037 2307 2143 2055 13961 19361 1005 1055 3945 2005 2370 3081 1996 8027 1997 2010 3297 4944 2269 1010 3434 19361 1010 2757 2144 3326 1012 1996 2143 16473 3254 1010 2021 6669 1010 1998 2054 4627 2041 2004 1037 9428 2439 5659 14045 20744 1005 1055 4767 5325 4895 10371 2015 2046 1037 3376 6925 2007 2172 6748 3574 2007 7634 2000 1996 5197 1997 2293 1010 3279 1010 2155 1998 3383 2062 14780 1010 2256 2166 1005 1055 2147 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2018 2196 2657 1997 3434 19361 3188 2000 2023 2143 1010 2348 1045 2001 15221 5220 2007 2070 1997 2010 2147 1012 2083 1996 2616 1006 2119 2204 1998 2919 1007 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ccp5trMwRtmr","colab_type":"text"},"source":["#Creating a model\n","\n","Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."]},{"cell_type":"code","metadata":{"id":"6o2a5ZIvRcJq","colab_type":"code","colab":{}},"source":["def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","  \"\"\"Creates a classification model.\"\"\"\n","\n","  bert_module = hub.Module(\n","      BERT_MODEL_HUB,\n","      trainable=True)\n","  bert_inputs = dict(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids)\n","  bert_outputs = bert_module(\n","      inputs=bert_inputs,\n","      signature=\"tokens\",\n","      as_dict=True)\n","\n","  # Use \"pooled_output\" for classification tasks on an entire sentence.\n","  # Use \"sequence_outputs\" for token-level output.\n","  output_layer = bert_outputs[\"pooled_output\"]\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  # Create our own layer to tune for politeness data.\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","\n","    # Dropout helps prevent overfitting\n","    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    # Convert labels into one-hot encoding\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","    # If we're predicting, we want predicted labels and the probabiltiies.\n","    if is_predicting:\n","      return (predicted_labels, log_probs)\n","\n","    # If we're train/eval, compute loss between predicted and actual label\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","    return (loss, predicted_labels, log_probs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpE0ZIDOCQzE","colab_type":"text"},"source":["Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."]},{"cell_type":"code","metadata":{"id":"FnH-AnOQ9KKW","colab_type":"code","colab":{}},"source":["# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","\n","    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","    \n","    # TRAIN and EVAL\n","    if not is_predicting:\n","\n","      (loss, predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      train_op = bert.optimization.create_optimizer(\n","          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","      # Calculate evaluation metrics. \n","      def metric_fn(label_ids, predicted_labels):\n","        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predicted_labels)\n","        auc = tf.metrics.auc(\n","            label_ids,\n","            predicted_labels)\n","        recall = tf.metrics.recall(\n","            label_ids,\n","            predicted_labels)\n","        precision = tf.metrics.precision(\n","            label_ids,\n","            predicted_labels) \n","        true_pos = tf.metrics.true_positives(\n","            label_ids,\n","            predicted_labels)\n","        true_neg = tf.metrics.true_negatives(\n","            label_ids,\n","            predicted_labels)   \n","        false_pos = tf.metrics.false_positives(\n","            label_ids,\n","            predicted_labels)  \n","        false_neg = tf.metrics.false_negatives(\n","            label_ids,\n","            predicted_labels)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"f1_score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","          loss=loss,\n","          train_op=train_op)\n","      else:\n","          return tf.estimator.EstimatorSpec(mode=mode,\n","            loss=loss,\n","            eval_metric_ops=eval_metrics)\n","    else:\n","      (predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      predictions = {\n","          'probabilities': log_probs,\n","          'labels': predicted_labels\n","      }\n","      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  # Return the actual model function in the closure\n","  return model_fn\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjwJ4bTeWXD8","colab_type":"code","colab":{}},"source":["# Compute train and warmup steps from batch size\n","# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n","BATCH_SIZE = 32\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 3.0\n","# Warmup is a period of time where hte learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"emHf9GhfWBZ_","colab_type":"code","colab":{}},"source":["# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEJldMr3WYZa","colab_type":"code","colab":{}},"source":["# Specify outpit directory and number of checkpoint steps to save\n","run_config = tf.estimator.RunConfig(\n","    model_dir=OUTPUT_DIR,\n","    save_summary_steps=SAVE_SUMMARY_STEPS,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_WebpS1X97v","colab_type":"code","outputId":"6b108a25-27b2-4769-e802-5ee4743fd27e","executionInfo":{"status":"ok","timestamp":1573606809574,"user_tz":-660,"elapsed":141228,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":283}},"source":["model_fn = model_fn_builder(\n","  num_labels=len(label_list),\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps)\n","\n","estimator = tf.estimator.Estimator(\n","  model_fn=model_fn,\n","  config=run_config,\n","  params={\"batch_size\": BATCH_SIZE})\n"],"execution_count":45,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'gs://sst2-ads-project/OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2439251908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'gs://sst2-ads-project/OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2439251908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"NOO3RfG1DYLo","colab_type":"text"},"source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."]},{"cell_type":"code","metadata":{"id":"1Pv2bAlOX_-K","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t6Nukby2EB6-","colab_type":"text"},"source":["Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."]},{"cell_type":"code","metadata":{"id":"nucD4gluYJmK","colab_type":"code","outputId":"42a91a3c-145d-442f-f326-6d055c82ea6a","executionInfo":{"status":"ok","timestamp":1573606812944,"user_tz":-660,"elapsed":144580,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Beginning Training!\n","INFO:tensorflow:Skipping training since max_steps has already saved.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Skipping training since max_steps has already saved.\n"],"name":"stderr"},{"output_type":"stream","text":["Training took time  0:00:03.155643\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CmbLTVniARy3","colab_type":"text"},"source":["Now let's use our test data to see how well our model did:"]},{"cell_type":"code","metadata":{"id":"JIhejfpyJ8Bx","colab_type":"code","colab":{}},"source":["test_input_fn = run_classifier.input_fn_builder(\n","    features=test_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPVEXhNjYXC-","colab_type":"code","outputId":"e2246e01-159f-434e-a452-4f190da5af34","executionInfo":{"status":"ok","timestamp":1573606954279,"user_tz":-660,"elapsed":285897,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["estimator.evaluate(input_fn=test_input_fn, steps=None)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-40-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-40-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-11-13T01:00:25Z\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-11-13T01:00:25Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from gs://sst2-ads-project/OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from gs://sst2-ads-project/OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-11-13-01:02:24\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-11-13-01:02:24\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 468: auc = 0.86662143, eval_accuracy = 0.8666, f1_score = 0.86689276, false_negatives = 344.0, false_positives = 323.0, global_step = 468, loss = 0.54360634, precision = 0.8705411, recall = 0.86327505, true_negatives = 2161.0, true_positives = 2172.0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 468: auc = 0.86662143, eval_accuracy = 0.8666, f1_score = 0.86689276, false_negatives = 344.0, false_positives = 323.0, global_step = 468, loss = 0.54360634, precision = 0.8705411, recall = 0.86327505, true_negatives = 2161.0, true_positives = 2172.0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: gs://sst2-ads-project/OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: gs://sst2-ads-project/OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'auc': 0.86662143,\n"," 'eval_accuracy': 0.8666,\n"," 'f1_score': 0.86689276,\n"," 'false_negatives': 344.0,\n"," 'false_positives': 323.0,\n"," 'global_step': 468,\n"," 'loss': 0.54360634,\n"," 'precision': 0.8705411,\n"," 'recall': 0.86327505,\n"," 'true_negatives': 2161.0,\n"," 'true_positives': 2172.0}"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"ueKsULteiz1B","colab_type":"text"},"source":["Now let's write code to make predictions on new sentences:"]},{"cell_type":"code","metadata":{"id":"OsrbTD2EJTVl","colab_type":"code","colab":{}},"source":["def getPrediction(in_sentences):\n","  labels = [\"Negative\", \"Positive\"]\n","  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n","  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n","  predictions = estimator.predict(predict_input_fn)\n","  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNjMw7BiNsEW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"4dc9aafb-4c72-44d1-ac28-088209cd06cf","executionInfo":{"status":"ok","timestamp":1573606956866,"user_tz":-660,"elapsed":288446,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","!ls \"/content/drive/My Drive/Colab Notebooks\"\n","pred_file = open(\"/content/drive/My Drive/Colab Notebooks/New_data.txt\",\"r\")"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","'Copy of Predicting Movie Reviews with BERT on TF Hub.ipynb'   Untitled1.ipynb\n"," New_data.txt\t\t\t\t\t\t       Untitled2.ipynb\n"," Untitled0.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rb2EFQaRQA_h","colab_type":"code","colab":{}},"source":["pred_sentences = pred_file.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ejx6RdG5Ss-S","colab_type":"code","colab":{}},"source":["pred_sentences = [\n","  \"I really like this movie.\", \n","  \"I hated watching this!\"\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrZmvZySKQTm","colab_type":"code","outputId":"a47c4eb4-1a80-4c44-8dee-84a6411f97bf","executionInfo":{"status":"ok","timestamp":1573607178423,"user_tz":-660,"elapsed":30458,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":811}},"source":["predictions = getPrediction(pred_sentences)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 2\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 2\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i really like this movie . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i really like this movie . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2428 2066 2023 3185 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2428 2066 2023 3185 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i hated watching this ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i hated watching this ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 6283 3666 2023 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 6283 3666 2023 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from gs://sst2-ads-project/OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from gs://sst2-ads-project/OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MXkRiEBUqN3n","colab_type":"text"},"source":["Voila! We have a sentiment classifier!"]},{"cell_type":"code","metadata":{"id":"ERkTE8-7oQLZ","colab_type":"code","outputId":"00cbd65b-1284-4a4e-e237-559c3eeed0d7","executionInfo":{"status":"ok","timestamp":1573607179473,"user_tz":-660,"elapsed":978,"user":{"displayName":"M.V Nikhil Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCNtDOasuplEGCq4a8UIkxxLg1oDQWr5HgQSYMZ9EM=s64","userId":"01499880190104623145"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["predictions"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('I really like this movie.',\n","  array([-6.1968870e+00, -2.0378316e-03], dtype=float32),\n","  'Positive'),\n"," ('I hated watching this!',\n","  array([-0.0053502, -5.233288 ], dtype=float32),\n","  'Negative')]"]},"metadata":{"tags":[]},"execution_count":57}]}]}